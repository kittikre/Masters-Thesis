{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a47fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Fri May  6 13:51:53 2022\n",
    "\n",
    "@author: Kitti\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.util import np_to_th\n",
    "from braindecode.models.modules import Expression, Ensure4d\n",
    "from braindecode.models.functions import (\n",
    "    safe_log, square, transpose_time_to_spat\n",
    ")\n",
    "from TransformFunction import reshape_lstm_input, extract_output_from_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowCNNLSTM(nn.Sequential):\n",
    "    \"\"\"Shallow ConvNet model from Schirrmeister et al 2017.\n",
    "\n",
    "    Model described in [Schirrmeister2017]_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_chans : int\n",
    "        XXX\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [Schirrmeister2017] Schirrmeister, R. T., Springenberg, J. T., Fiederer,\n",
    "       L. D. J., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F.\n",
    "       & Ball, T. (2017).\n",
    "       Deep learning with convolutional neural networks for EEG decoding and\n",
    "       visualization.\n",
    "       Human Brain Mapping , Aug. 2017.\n",
    "       Online: http://dx.doi.org/10.1002/hbm.23730\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans,\n",
    "        n_classes,\n",
    "        input_window_samples=None,\n",
    "        n_filters_time=40,\n",
    "        filter_time_length=25,\n",
    "        n_filters_spat=40,\n",
    "        pool_time_length=75,\n",
    "        pool_time_stride=15,\n",
    "        final_conv_length=30,\n",
    "        conv_nonlin=square,\n",
    "        pool_mode=\"mean\",\n",
    "        pool_nonlin=safe_log,\n",
    "        split_first_layer=True,\n",
    "        batch_norm=True,\n",
    "        batch_norm_alpha=0.1,\n",
    "        drop_prob=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if final_conv_length == \"auto\":\n",
    "            assert input_window_samples is not None\n",
    "        self.in_chans = in_chans\n",
    "        self.n_classes = n_classes\n",
    "        self.input_window_samples = input_window_samples\n",
    "        self.n_filters_time = n_filters_time\n",
    "        self.filter_time_length = filter_time_length\n",
    "        self.n_filters_spat = n_filters_spat\n",
    "        self.pool_time_length = pool_time_length\n",
    "        self.pool_time_stride = pool_time_stride\n",
    "        self.final_conv_length = final_conv_length\n",
    "        self.conv_nonlin = conv_nonlin\n",
    "        self.pool_mode = pool_mode\n",
    "        self.pool_nonlin = pool_nonlin\n",
    "        self.split_first_layer = split_first_layer\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_alpha = batch_norm_alpha\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.add_module(\"ensuredims\", Ensure4d())\n",
    "        pool_class = dict(max=nn.MaxPool2d, mean=nn.AvgPool2d)[self.pool_mode]\n",
    "        if self.split_first_layer:\n",
    "            self.add_module(\"dimshuffle\", Expression(transpose_time_to_spat))\n",
    "            self.add_module(\n",
    "                \"conv_time\",\n",
    "                nn.Conv2d(\n",
    "                    1,\n",
    "                    self.n_filters_time,\n",
    "                    (self.filter_time_length, 1),\n",
    "                    stride=1,\n",
    "                ),\n",
    "            )\n",
    "            self.add_module(\n",
    "                \"conv_spat\",\n",
    "                nn.Conv2d(\n",
    "                    self.n_filters_time,\n",
    "                    self.n_filters_spat,\n",
    "                    (1, self.in_chans),\n",
    "                    stride=1,\n",
    "                    bias=not self.batch_norm,\n",
    "                    #padding=\"same\",\n",
    "                \n",
    "                ),\n",
    "            )\n",
    "            n_filters_conv = self.n_filters_spat\n",
    "        else:\n",
    "            self.add_module(\n",
    "                \"conv_time\",\n",
    "                nn.Conv2d(\n",
    "                    self.in_chans,\n",
    "                    self.n_filters_time,\n",
    "                    (self.filter_time_length, 1),\n",
    "                    stride=1,\n",
    "                    bias=not self.batch_norm,\n",
    "                    #padding=\"same\",\n",
    "                ),\n",
    "            )\n",
    "            n_filters_conv = self.n_filters_time\n",
    "        if self.batch_norm:\n",
    "            self.add_module(\n",
    "                \"bnorm\",\n",
    "                nn.BatchNorm2d(\n",
    "                    n_filters_conv, momentum=self.batch_norm_alpha, affine=True\n",
    "                ),\n",
    "            )\n",
    "        self.add_module(\"conv_nonlin_exp\", Expression(self.conv_nonlin))\n",
    "        self.add_module(\n",
    "            \"pool\",\n",
    "            pool_class(\n",
    "                kernel_size=(self.pool_time_length, 1),\n",
    "                stride=(self.pool_time_stride, 1),\n",
    "            ),\n",
    "        )\n",
    "        self.add_module(\"pool_nonlin_exp\", Expression(self.pool_nonlin))\n",
    "        self.add_module(\"drop\", nn.Dropout(p=self.drop_prob))\n",
    "        self.eval()\n",
    "        if self.final_conv_length == \"auto\":\n",
    "            out = self(\n",
    "                np_to_th(\n",
    "                    np.ones(\n",
    "                        (1, self.in_chans, self.input_window_samples, 1),\n",
    "                        dtype=np.float32,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            n_out_time = out.cpu().data.numpy().shape[2]\n",
    "            self.final_conv_length = n_out_time\n",
    "        self.add_module(\n",
    "            \"conv_classifier\",\n",
    "            nn.Conv2d(\n",
    "                n_filters_conv,\n",
    "                self.n_classes,\n",
    "                (self.final_conv_length, 1),\n",
    "                bias=True,\n",
    "                #padding=\"same\",\n",
    "            ),\n",
    "        )\n",
    "        self.add_module(\"reshapeinput\", Expression(reshape_lstm_input))\n",
    "\n",
    "        self.add_module(\n",
    "            \"LSTM\", \n",
    "            nn.LSTM(740, 370, 1,\n",
    "                batch_first=True\n",
    "            ),\n",
    "        ) \n",
    "        \n",
    "        self.add_module(\"extract_lstm\", Expression(extract_output_from_lstm))\n",
    "        \n",
    "        self.add_module(\n",
    "            \"Linear\",\n",
    "             nn.Linear(\n",
    "                 in_features= 370,\n",
    "                 out_features = 2,\n",
    "             ),\n",
    "        )\n",
    "        \n",
    "        # self.add_module(\"softmax\", nn.LogSoftmax(dim=1))\n",
    "        \n",
    "        #self.add_module(\"squeeze\", Expression(squeeze_final_output))\n",
    "        \n",
    "\n",
    "        \n",
    "        # Initialization, xavier is same as in paper...\n",
    "        init.xavier_uniform_(self.conv_time.weight, gain=1)\n",
    "        # maybe no bias in case of no split layer and batch norm\n",
    "        if self.split_first_layer or (not self.batch_norm):\n",
    "            init.constant_(self.conv_time.bias, 0)\n",
    "        if self.split_first_layer:\n",
    "            init.xavier_uniform_(self.conv_spat.weight, gain=1)\n",
    "            if not self.batch_norm:\n",
    "                init.constant_(self.conv_spat.bias, 0)\n",
    "        if self.batch_norm:\n",
    "            init.constant_(self.bnorm.weight, 1)\n",
    "            init.constant_(self.bnorm.bias, 0)\n",
    "        init.xavier_uniform_(self.conv_classifier.weight, gain=1)\n",
    "        init.constant_(self.conv_classifier.bias, 0)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
