{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e11a2f6-ae0b-47a3-8171-8fa42c7b48f0",
   "metadata": {},
   "source": [
    "This is the main script for the training and testing of the hybrid deepCNN-LSTM model,\n",
    "used for the classification of abnormal EEG activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96cbdc-8754-401d-8bce-e9cc3d642038",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a83be",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import numpy as np\n",
    "from PreProcessing import import_tuh_abnormal, filter_only_adults, create_ch_mapping\n",
    "from DataTransform import data_transform, train_valid_test_split, get_parameters_for_model\n",
    "from TrainingValidationTesting_HybridBCELoss import train_model, test_model\n",
    "import torch\n",
    "from braindecode.preprocessing import preprocess, Preprocessor, scale as multiply\n",
    "from Models import create_model\n",
    "from braindecode.util import set_random_seeds\n",
    "from torch import optim\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Importing TUH Abnormal dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d7924c-0268-4c4d-b74e-66f46930e790",
   "metadata": {},
   "source": [
    "## Import TUH Abnormal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde645f",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "TUH_PATH = 'TUH EEG Abnormal dataset/Abnormal/' # specify the path to the TUH Abnormal dataset\n",
    "N_JOBS = 64  # specify the number of jobs for loading and windowing\n",
    "\n",
    "# Parameters to be defined for the preprocessing pipeline\n",
    "TMIN = 1 * 60\n",
    "TMAX = 2.5 * 60 \n",
    "SFREQ = 100\n",
    "CLIPPING = 800\n",
    "\n",
    "tuh_abnormal = import_tuh_abnormal(TUH_PATH, N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd14fc6-b547-415c-ae9f-c8844e0a7446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter for adults\n",
    "\n",
    "tuh_abnormal_adults = filter_only_adults(tuh_abnormal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea60a44",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create short channel names and mapping    \n",
    "short_ch_names, ch_mapping = create_ch_mapping()\n",
    "\n",
    "def select_by_channels(ds, short_ch_names, ch_mapping):\n",
    "    # Function for selecting channels defined in the input.\n",
    "        \n",
    "    split_ids = []\n",
    "    for i, d in enumerate(ds.datasets):\n",
    "        ref = 'ar' if d.raw.ch_names[0].endswith('-REF') else 'le'\n",
    "        # these are the channels we are looking for\n",
    "        seta = set(ch_mapping[ref].keys())\n",
    "        # these are the channels of the recoding\n",
    "        setb = set(d.raw.ch_names)\n",
    "        # if recording contains all channels we are looking for, include it\n",
    "        if seta.issubset(setb):\n",
    "            split_ids.append(i)\n",
    "    return ds.split(split_ids)['0']\n",
    "\n",
    "\n",
    "def custom_crop(raw, tmin=0.0, tmax=None, include_tmax=True):       \n",
    "    # Crop recordings to tmin â€“ tmax. can be incomplete if recording has lower duration than tmax\n",
    "    \n",
    "    tmax = min((raw.n_times - 1) / raw.info['sfreq'], tmax)\n",
    "    raw.crop(tmin=tmin, tmax=tmax, include_tmax=include_tmax)\n",
    "    \n",
    "    \n",
    "def custom_rename_channels(raw, mapping):    \n",
    "    # Rename channels which are dependent on referencing:\n",
    "    # le: EEG 01-LE, ar: EEG 01-REF\n",
    "    \n",
    "    reference = raw.ch_names[0].split('-')[-1].lower()\n",
    "    assert reference in ['le', 'ref'], 'unexpected referencing'\n",
    "    reference = 'le' if reference == 'le' else 'ar'\n",
    "    raw.rename_channels(mapping[reference])\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessors = [\n",
    "        Preprocessor(custom_crop, tmin=TMIN, tmax=TMAX, include_tmax=True,\n",
    "                     apply_on_array=False),\n",
    "        Preprocessor('set_eeg_reference', ref_channels='average', ch_type='eeg'), # mne Raw class function\n",
    "        Preprocessor(custom_rename_channels, mapping=ch_mapping, # rename channels to short channel names\n",
    "                     apply_on_array=False), #\n",
    "        Preprocessor('pick_channels', ch_names=short_ch_names, ordered=True), # mne Raw class function\n",
    "        Preprocessor(multiply, factor=1e6, apply_on_array=True), # scaling signals to microvolt\n",
    "        Preprocessor(np.clip, a_min=-CLIPPING, a_max=CLIPPING, apply_on_array=True), # clip outlier values to +/- 800 micro volts\n",
    "        Preprocessor('resample', sfreq=SFREQ), # mne Raw class function\n",
    "    ]\n",
    "\n",
    "\n",
    "tuh_preproc = preprocess(\n",
    "        concat_ds=tuh_abnormal, #input either tuh_abnormal or tuh_abnormal_adults\n",
    "        preprocessors=preprocessors,\n",
    "        n_jobs=N_JOBS,\n",
    "        save_dir=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ecfd7-0674-4b3f-b953-a5de8c462d13",
   "metadata": {},
   "source": [
    "## Transform Data and Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4279b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute train-validation-test split\n",
    "\n",
    "TRAIN_SIZE=0.9 # Parameter for defining the ratio of training set (remainder defines the size of the validation set)\n",
    "\n",
    "tuh_train, tuh_val, tuh_test = train_valid_test_split(tuh_preproc, train_size=TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0beec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True, use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "# Required parameters for model creation\n",
    "INPUT_WIN_SAMPLES = 6000\n",
    "IN_CHANS, INPUT_SIZE_SAMPLES = get_parameters_for_model(tuh_train)\n",
    "N_CLASSES = 2\n",
    "MODEL_NAME = \"hybrid\" # it can either be shallow, deep, or hybrid\n",
    "FINAL_CONV_LENGTH = 1\n",
    "\n",
    "model = create_model(MODEL_NAME, IN_CHANS, N_CLASSES, INPUT_WIN_SAMPLES, FINAL_CONV_LENGTH)\n",
    "\n",
    "\n",
    "# If available, use multiple GPUs for training and testing\n",
    "if cuda:\n",
    "    device = \"cuda:0\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids = [ 0, 1])\n",
    "model.to(device)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6dd32b",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute DataLoader generating function\n",
    "\n",
    "BATCH_SIZE=64\n",
    "\n",
    "train_loader, val_loader, test_loader = data_transform(tuh_train, tuh_val, tuh_test, model, N_JOBS, train_size=TRAIN_SIZE, batch_size=BATCH_SIZE, input_window_samples=INPUT_WIN_SAMPLES, in_chans = IN_CHANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902529a",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters required for the training of the model\n",
    "\n",
    "lr =  0.01\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "criterion = torch.nn.BCELoss()\n",
    "n_epochs = 1 \n",
    "patience = 1 #ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb3c60f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "trained_model, train_losses, train_accs, valid_losses, valid_accs = train_model(model, device, criterion, optimizer, train_loader, val_loader, n_epochs, patience)\n",
    "trained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb30370-1df2-4c5f-8ed0-0ae75cb912ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "\n",
    "PATH = './modelname.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8dc021",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Model testing\n",
    "\n",
    "x_test, y_test, y_pred, accuracy = test_model(device, trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e7238-84c1-49c5-9fb4-94ff83a102d0",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760b4ec",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Transform tensors from test_model"
   },
   "outputs": [],
   "source": [
    "# Reshape tensors\n",
    "\n",
    "x_test = x_test.flatten().cpu()\n",
    "y_test = y_test.flatten().cpu()\n",
    "y_pred = y_pred.flatten().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd93a9f",
   "metadata": {
    "title": "Training losses and accuracy plot"
   },
   "outputs": [],
   "source": [
    "# Training loss and accuracy per epoch\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(train_losses, marker=\"o\", color=\"orangered\", label=\"Training loss\")\n",
    "ax.set_xlabel(\"Number of epochs\")\n",
    "ax.set_ylabel(\"Loss\", color=\"orangered\")\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(train_accs, marker=\"o\", color=\"cornflowerblue\", label=\"Training accuracy\")\n",
    "ax2.set_ylabel(\"Accuracy\", color=\"cornflowerblue\")\n",
    "\n",
    "plt.title(\"Training loss and accuracy per epoch\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8886bcc-3811-4444-8f93-e0c913376e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curve with training and validation loss\n",
    "\n",
    "ax = plt.axes()\n",
    "plt.plot(train_losses, marker=\".\", color=\"cornflowerblue\", label='Training loss')\n",
    "plt.plot(valid_losses, marker=\".\", color=\"mediumseagreen\", label='Validation loss')\n",
    "plt.title('Training and Validation Losses', fontsize=14)\n",
    "plt.xlabel('Number of epochs', fontsize=14)\n",
    "plt.legend(frameon=False, fontsize=14)\n",
    "ax.tick_params(axis='both', labelsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e216ec",
   "metadata": {
    "tags": [],
    "title": "ROC Curve plot"
   },
   "outputs": [],
   "source": [
    "''' \n",
    "The ROC Curve plots two parameters: the True Positive Rate (TPR), a synonim to recall,\n",
    "and the False Positive Rate (FPR). \n",
    "The plot also shows the ROC AUC score.\n",
    "By default, the class considered as the positive (It means Abnormal in our case).\n",
    "\n",
    "'''\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred, pos_label=None)\n",
    "plt.title(\"ROC Curve\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f2016-3d48-4477-a75d-38fe0dbeef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[90]: Confusion matrix and Classification Report\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cl_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(cl_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1391b5ab",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [],
    "title": "Plot Confusion matrix"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix plot\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                conf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     conf_matrix.flatten()/np.sum(conf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "\n",
    "sns.heatmap(conf_matrix, annot=labels, fmt='', cmap='YlGn')\n",
    "sns.set(font_scale=1.6)\n",
    "ax.set_title('Confusion Matrix', fontsize=16)\n",
    "ax.set_xlabel('Predicted labels', fontsize=14)\n",
    "ax.set_ylabel('True labels', fontsize=14)\n",
    "ax.tick_params(axis='both', labelsize=13)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
