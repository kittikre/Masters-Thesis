{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246cb82b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Data functions for splitting the dataset into train-valid-test sets,\n",
    "creating fixed-length windows, and channeling the inputs into pytorch DataLoaders\n",
    "\n",
    "Each method has its own description in it's header section.'\n",
    "\n",
    "The methods defined in this file are:\n",
    "    - train_valid_test_split\n",
    "    - get_parameters_for_model\n",
    "    - create_windows\n",
    "    - transform_target\n",
    "    - create_dataloaders\n",
    "    - data_transform\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8e9d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from braindecode.datasets.tuh import TUHAbnormal, TUH\n",
    "from braindecode.models import get_output_shape\n",
    "from braindecode.preprocessing import create_fixed_length_windows\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30c84f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_valid_test_split(tuh_preproc, train_size=0.9):\n",
    "    \"\"\"\n",
    "    Function for splitting the dataset into train, validation, \n",
    "    and test sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tuh_preproc : preprocessed TUH Abnormal dataset\n",
    "    train_size : size of the training set (defines split between train-validation sets)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tuh_train : train set\n",
    "    tuh_val validation set\n",
    "    tuh_test : test set\n",
    "    \"\"\"\n",
    "    \n",
    "    train_test_splits = tuh_preproc.split(\"train\")\n",
    "    tuh_train, tuh_test = train_test_splits['True'], train_test_splits['False']\n",
    "\n",
    "    train_len = len(tuh_train.datasets)\n",
    "    train_size = train_size\n",
    "\n",
    "    tuh_train_inds = [*range(train_len)]\n",
    "    train_index = int(train_len*train_size)-1\n",
    "    new_train_inds = [*range(train_index)]\n",
    "\n",
    "\n",
    "    new_val_inds = list(set(tuh_train_inds)-set(new_train_inds))\n",
    "    \n",
    "    train_val_splits = tuh_train.split({'train': new_train_inds, 'val': new_val_inds})\n",
    "    tuh_train, tuh_val = train_val_splits['train'],train_val_splits['val']\n",
    "    \n",
    "    return tuh_train, tuh_val, tuh_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9b09b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_parameters_for_model(tuh_train):\n",
    "    \"\"\"\n",
    "    Function for extracting the number of channels, and\n",
    "    size of one input from the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tuh_train : training set\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    n_chans : number of channels\n",
    "    input_size_samples : size of an input in the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    n_chans, input_size_samples = tuh_train[0][0].shape\n",
    "    \n",
    "    return n_chans, input_size_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a31e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_windows(model, tuh_train, tuh_val, tuh_test, n_jobs, input_window_samples=6000, in_chans=21):\n",
    "    \"\"\"\n",
    "    Function for creating equal-sized windows from\n",
    "    the data. Based on create_fixed_length_windows\n",
    "    function from braindecode package.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : model to be trained later on the dataset\n",
    "    tuh_train : train set\n",
    "    tuh_val : validation set\n",
    "    tuh_test : test set\n",
    "    n_jobs : number of jobs used for parallel execution\n",
    "    input_window_samples : size of the windows to be created (in ms)\n",
    "    in_chans: number of channels in the dataset\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    train_set : train set containing the newly generated windows as data points\n",
    "    val_set : validation set containing the newly generated windows as data points\n",
    "    test_set : test set containing the newly generated windows as data points\n",
    "    n_preds_per_input : number of predictions per one input\n",
    "    \"\"\"\n",
    "    \n",
    "    n_preds_per_input = get_output_shape(model, in_chans, input_window_samples)[2] #\n",
    "    \n",
    "    train_set = create_fixed_length_windows(\n",
    "        tuh_train,\n",
    "        start_offset_samples=0,\n",
    "        stop_offset_samples=None,\n",
    "        window_size_samples=input_window_samples,\n",
    "        window_stride_samples=n_preds_per_input,\n",
    "        drop_last_window=False,\n",
    "        preload=False,\n",
    "        n_jobs=n_jobs,\n",
    "        mapping={False: 0, True: 1},  # map non-digit targets\n",
    "    )\n",
    "    \n",
    "    val_set = create_fixed_length_windows(\n",
    "        tuh_val,\n",
    "        start_offset_samples=0,\n",
    "        stop_offset_samples=None,\n",
    "        window_size_samples=input_window_samples,\n",
    "        window_stride_samples=n_preds_per_input,\n",
    "        drop_last_window=False,\n",
    "        preload=False,\n",
    "        n_jobs=n_jobs,\n",
    "        mapping={False: 0, True: 1},  # map non-digit targets\n",
    "    )\n",
    "    \n",
    "    test_set = create_fixed_length_windows(\n",
    "        tuh_test,\n",
    "        start_offset_samples=0,\n",
    "        stop_offset_samples=None,\n",
    "        window_size_samples=input_window_samples,\n",
    "        window_stride_samples=n_preds_per_input,\n",
    "        drop_last_window=False,\n",
    "        preload=False,\n",
    "        n_jobs=n_jobs,\n",
    "        mapping={False: 0, True: 1},  # map non-digit targets\n",
    "    )\n",
    "    \n",
    "    return train_set, val_set, test_set, n_preds_per_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd885020",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "def transform_target(train_set, val_set, test_set, n_preds_per_input): \n",
    "    \"\"\"\n",
    "    Function for transforming shape of the target data, to match with\n",
    "    the shape of inputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_set : training set\n",
    "    val_set : validation set\n",
    "    test_set : test set\n",
    "    n_preds_per_input : number of predictions per one input\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    -\n",
    "    \"\"\"\n",
    "    train_set.target_transform = lambda x: np.full((n_preds_per_input), fill_value=x)\n",
    "    val_set.target_transform = lambda x: np.full((n_preds_per_input), fill_value=x)\n",
    "    test_set.target_transform = lambda x: np.full((n_preds_per_input), fill_value=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb5e0c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloaders(train_set, val_set, test_set, batch_size=64):\n",
    "    \"\"\"\n",
    "    Function for generating PyTorch DataLoaders from the training,\n",
    "    validation and test sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_set : training set\n",
    "    val_set : validation set\n",
    "    test_set : test set\n",
    "    batch_size : batch size\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    train_loader : DataLoader containing the training set\n",
    "    val_loader : DataLoader containing the validation set\n",
    "    test_loader : DataLoader containing the test set\n",
    "    \n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ff0b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def data_transform(tuh_train, tuh_val, tuh_test, model, n_jobs, train_size=0.9, batch_size=64, input_window_samples=6000, in_chans=21):\n",
    "    \"\"\"\n",
    "    Wrapper function that executes the above-defined\n",
    "    functions all at once.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    train_loader : DataLoader containing the training set\n",
    "    val_loader : DataLoader containing the validation set\n",
    "    test_loader : DataLoader containing the test set\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    train_set, val_set, test_set, n_preds_per_input = create_windows(model, tuh_train, tuh_val, tuh_test, n_jobs, input_window_samples=input_window_samples, in_chans=in_chans)\n",
    "    transform_target(train_set, val_set, test_set, n_preds_per_input)\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_set, val_set, test_set, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
