{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996aa6e0-d7f8-4d6a-81b9-cc10163c25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcfdab78-7533-4ddc-8ffa-f0aeccb66d33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T16:37:50.648387Z",
     "iopub.status.busy": "2022-05-02T16:37:50.647620Z",
     "iopub.status.idle": "2022-05-02T16:37:50.668451Z",
     "shell.execute_reply": "2022-05-02T16:37:50.667675Z",
     "shell.execute_reply.started": "2022-05-02T16:37:50.648317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def train_model(model, device, criterion, optimizer, train_loader, valid_loader, n_epochs=5, early_stop_patience=2):\n",
    "    \n",
    "    model.train()\n",
    "    the_last_loss = 100\n",
    "    trigger_times = 0\n",
    "    train_losses_per_epoch, valid_losses_per_epoch = [], []\n",
    "    \n",
    "    # modify this later and find a nicer solution\n",
    "    for i in range(1, 10000):\n",
    "        if len(train_loader) / i <= 20:\n",
    "            print_every = i\n",
    "            break\n",
    "        \n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels, inds = data\n",
    "            inputs = inputs.float()\n",
    "            labels = torch.clone(labels.long())\n",
    "            \n",
    "            # sending input to GPU/CPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.numel()\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "          \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            train_acc = correct/total\n",
    "            running_acc += train_acc\n",
    "            \n",
    "            \n",
    "            if i % print_every == print_every-1 or i+1 == len(train_loader):\n",
    "                now = datetime.now()\n",
    "                current_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "                print('Time: {}. . [{}/{}, {}/{}] train_loss: {:.6}, train_acc: {:.3}'.format(current_time, epoch+1, n_epochs, i+1, len(train_loader), running_loss/(i+1), running_acc/(i+1)))\n",
    "    \n",
    "        train_losses_per_epoch.append(running_loss/len(train_loader))\n",
    "        current_loss = validate_model(model, device, valid_loader, criterion)\n",
    "        print('The Current Validation Loss:', current_loss)\n",
    "            \n",
    "        if current_loss > the_last_loss:\n",
    "            trigger_times += 1\n",
    "            print('Trigger Times:', trigger_times)\n",
    "    \n",
    "            if trigger_times >= early_stop_patience:\n",
    "                print('Early Stopping!\\nStart the test process.')\n",
    "                return model, train_losses_per_epoch, valid_losses_per_epoch\n",
    "                # this needs to be modified to start the testing process -> return model if turn into a function\n",
    "    \n",
    "        else:\n",
    "            print('Trigger Times: 0')\n",
    "            trigger_times = 0\n",
    "    \n",
    "        the_last_loss = current_loss\n",
    "        valid_losses_per_epoch.append(current_loss)\n",
    "    \n",
    "    print('Finished Training')        \n",
    "    return model, train_losses_per_epoch, valid_losses_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c24cd7-e0b2-48bf-a809-73e5bc68bd98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T16:38:22.082628Z",
     "iopub.status.busy": "2022-05-02T16:38:22.081356Z",
     "iopub.status.idle": "2022-05-02T16:38:22.098305Z",
     "shell.execute_reply": "2022-05-02T16:38:22.097501Z",
     "shell.execute_reply.started": "2022-05-02T16:38:22.082552Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate_model(model, device, valid_loader, loss_function):\n",
    "\n",
    "     model.eval()\n",
    "     loss_total = 0\n",
    "\n",
    "     # Test validation data\n",
    "     with torch.no_grad():\n",
    "         for data in valid_loader:\n",
    "             inputs, labels, inds = data\n",
    "             inputs = inputs.float()\n",
    "             labels = torch.clone(labels.long())\n",
    "             \n",
    "             #Sending input to GPU/CPU\n",
    "             inputs, labels = inputs.to(device),  labels.to(device)\n",
    "             \n",
    "\n",
    "             output = model(inputs)\n",
    "             loss = loss_function(output, labels)\n",
    "             loss_total += loss.item()\n",
    "\n",
    "     return loss_total / len(valid_loader)  \n",
    "\n",
    "    \n",
    "# Test function\n",
    "\n",
    "def test_model(device, model, test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    accuracy = 0\n",
    "    labels_all = torch.tensor([])\n",
    "    predicted_all = torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels, inds = data\n",
    "            inputs = inputs.float()\n",
    "            labels = torch.clone(labels.long())\n",
    "            \n",
    "            # sending input to GPU/CPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "            total += labels.numel()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            \n",
    "            ps = torch.exp(output)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals =  top_class == labels.view(*top_class.shape)\n",
    "            accuracy +=   torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            predicted_all = torch.cat((predicted_all, predicted) ,dim=0)\n",
    "            labels_all = torch.cat((labels_all, labels) ,dim=0)\n",
    "            \n",
    "\n",
    "    print('Accuracy: {:.2}'.format(correct/total))\n",
    "    print('Accuracy2: {:.2}'.format(accuracy/len(test_loader)))\n",
    "    return labels_all, predicted_all\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
