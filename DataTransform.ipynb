{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead5a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246cb82b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Created on Mon May  2 17:36:56 2022\n",
    "\n",
    "Data functions for splitting the dataset into train-valid-test sets,\n",
    "creating fixed-length windows, and channeling the inputs into pytorch DataLoaders\n",
    "\n",
    "Each method has its own description in it's header section.'\n",
    "\n",
    "The methods defined in this file are:\n",
    "    \n",
    "\n",
    "@author: Kitti\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8e9d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from braindecode.datasets.tuh import TUHAbnormal, TUH\n",
    "from braindecode.models import get_output_shape\n",
    "from braindecode.preprocessing import create_fixed_length_windows\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30c84f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_valid_test_split(tuh_preproc, train_size=0.9):\n",
    "    train_test_splits = tuh_preproc.split(\"train\")\n",
    "    tuh_train, tuh_test = train_test_splits['True'], train_test_splits['False']\n",
    "\n",
    "    train_len = len(tuh_train.datasets)\n",
    "    train_size = train_size\n",
    "\n",
    "    tuh_train_inds = [*range(train_len)]\n",
    "    train_index = int(train_len*train_size)-1\n",
    "    new_train_inds = [*range(train_index)]\n",
    "\n",
    "\n",
    "    new_val_inds = list(set(tuh_train_inds)-set(new_train_inds))\n",
    "    \n",
    "    train_val_splits = tuh_train.split({'train': new_train_inds, 'val': new_val_inds})\n",
    "    tuh_train, tuh_val = train_val_splits['train'],train_val_splits['val']\n",
    "    \n",
    "    return tuh_train, tuh_val, tuh_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9b09b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Rename it later\n",
    "\n",
    "def get_parameters_for_model(tuh_train):\n",
    "    n_chans, input_size_samples = tuh_train[0][0].shape\n",
    "    \n",
    "    return n_chans, input_size_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a31e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create windows using braindecode function for this. It needs parameters to define how\n",
    "# trials should be used.\n",
    "\n",
    "def create_windows(model, tuh_train, tuh_val, tuh_test, n_jobs, input_window_samples=6000, in_chans=21):\n",
    "\n",
    "    n_preds_per_input = get_output_shape(model, in_chans, input_window_samples)[2] #\n",
    "    \n",
    "    train_set = create_fixed_length_windows(\n",
    "        tuh_train,\n",
    "        start_offset_samples=0,\n",
    "        stop_offset_samples=None,\n",
    "        window_size_samples=input_window_samples,\n",
    "        window_stride_samples=n_preds_per_input,\n",
    "        drop_last_window=False,\n",
    "        preload=False,\n",
    "        n_jobs=n_jobs,\n",
    "        mapping={False: 0, True: 1},  # map non-digit targets\n",
    "    )\n",
    "    \n",
    "    val_set = create_fixed_length_windows(\n",
    "        tuh_val,\n",
    "        start_offset_samples=0,\n",
    "        stop_offset_samples=None,\n",
    "        window_size_samples=input_window_samples,\n",
    "        window_stride_samples=n_preds_per_input,\n",
    "        drop_last_window=False,\n",
    "        preload=False,\n",
    "        n_jobs=n_jobs,\n",
    "        mapping={False: 0, True: 1},  # map non-digit targets\n",
    "    )\n",
    "    \n",
    "    test_set = create_fixed_length_windows(\n",
    "        tuh_test,\n",
    "        start_offset_samples=0,\n",
    "        stop_offset_samples=None,\n",
    "        window_size_samples=input_window_samples,\n",
    "        window_stride_samples=n_preds_per_input,\n",
    "        drop_last_window=False,\n",
    "        preload=False,\n",
    "        n_jobs=n_jobs,\n",
    "        mapping={False: 0, True: 1},  # map non-digit targets\n",
    "    )\n",
    "    \n",
    "    return train_set, val_set, test_set, n_preds_per_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd885020",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def transform_target(train_set, val_set, test_set, n_preds_per_input): \n",
    "    train_set.target_transform = lambda x: np.full((n_preds_per_input), fill_value=x)\n",
    "    val_set.target_transform = lambda x: np.full((n_preds_per_input), fill_value=x)\n",
    "    test_set.target_transform = lambda x: np.full((n_preds_per_input), fill_value=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb5e0c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_dataloaders(train_set, val_set, test_set, batch_size=64):\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ff0b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def data_transform(tuh_train, tuh_val, tuh_test, model, n_jobs, train_size=0.9, batch_size=64, input_window_samples=6000, in_chans=21):\n",
    "\n",
    "    train_set, val_set, test_set, n_preds_per_input = create_windows(model, tuh_train, tuh_val, tuh_test, n_jobs, input_window_samples=input_window_samples, in_chans=in_chans)\n",
    "    transform_target(train_set, val_set, test_set, n_preds_per_input)\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_set, val_set, test_set, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
